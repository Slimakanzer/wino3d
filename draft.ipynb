{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Winograd algorithm for 3D convolution\n",
    "## Background\n",
    "\n",
    "The Winograd family of fast convolution algorithms attempts to minimize the number of operations needed for fixed-size small convolutions. Around 1980, Winograd proved that a convolution of the input of length $n$ with a kernel of length $n_{h}$ can be computed using a theoretical minimum of just $n+n_{h}-1$ general multiplications.\n",
    "\n",
    "However, most studies actually used Toom-Cook algorithm and described their approach as \"Winograd convolution\" and within the DNN research literature that term has come to include both Toom-Cook and Winograd methods. Toom-Cook convolution algorithm based on the Chinese Reminder Theorem for polynomials and the Matrix Exchange Theorem.\n",
    "\n",
    "**Toom-Cook algorithm.** The one-dimensional discrete convolution of two vectors $h[h_{1}..h_{n_{h}}]$ and $x[x_{1}..x_{n}]$ is the vector $s = h*x$ where $s_{i}= \\sum \\limits_{j=1}^{i} h_{j}*x_{i-j}$ . Using Toom-Cook algorithm we can present it as $(h*x)=V^{-1}(V_{x}x \\odot V_{h}h)$ where $V_{x}$ and $V_{h}$ are *Vandermonde* matrices to transform values into the *modulo polynomial domain* and evaluate polynomials in different $p_{i}$ points, and $V^{-1}$ is the *inverse Vandermonde* matrix for the transformation back from the *modulo polynomial domain*.\n",
    "\n",
    "$$ (h*x) = V^{-1}(V_{x}x \\odot V_{h}h) = V_{x}^{T}(V_{h}h \\odot V^{-T}x) $$\n",
    "\n",
    "Let $A=V_{x}, G=V_{h}, B=V^{-1}$\n",
    "\n",
    "$$ (h*x)_{1D} = A^{T}(Gh \\odot B^{T}x) $$\n",
    "\n",
    "In similar way for 2D-convolution:\n",
    "$$ (h*x)_{2D} = A^{T}(GhG^{T} \\odot B^{T}xB)A $$\n",
    "\n",
    "**N-mode Tensor-Matrix product.** *The n-mode Tensor-Matrix product* of a tensor $X \\in \\R^{I_{1} \\times I_{2} \\times ... \\times I_{N}}$ with a matrix $U \\in \\R^{J \\times I_{n}}$ is denoted by $M = X  \\times_{n} U$ and $M \\in \\R^{I_{1} \\times ...J \\times ... \\times I_{N}}$.\n",
    "\n",
    "$$(X  \\times_{n} U)_{i_{1}...j...i_{N}} = \\sum \\limits_{i_{n}=1}^{I_{n}} X_{i_{1}...i_{n}...i_{N}} \\times U_{ji_{n}}$$\n",
    "\n",
    "For matrices *X* and *U* n-mode Tensor-Matrix product is equivalent to:\n",
    "$$ X  \\times_{1} U = UX$$\n",
    "$$ X  \\times_{2} U = XU^{T}$$\n",
    "$$ (X  \\times_{1} U) \\times_{2} = X  \\times_{1} U \\times_{2} = UXU^{T}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "\n",
    "def nmod_tensor_product(A, U, n):\n",
    "    \"\"\"\n",
    "    n-Mode Tensor - Matrix Product\n",
    "    ref: https://arxiv.org/pdf/1611.06565.pdf\n",
    "\n",
    "    Input:\n",
    "    A: tensor of R^( I_1 × I_2 × .. I_n × .. I_N )\n",
    "    U: matrix of R^( J × I_n)\n",
    "    n: scalar within [1:N], specifying the mode\n",
    "\n",
    "    Output:\n",
    "    B: output tensor of R^(I_1 × I_2 × .. J × .. I_N)\n",
    "\n",
    "    Calculation:\n",
    "    M[i1,i2,..,j,..,iN] = sum(A[i1,i2,..,in,..,iN] * U[j,in]) by axis 'in'\n",
    "    \"\"\"\n",
    "    assert n > 0 and n <= len(A.shape)\n",
    "    assert A.shape[n-1] == U.shape[1]\n",
    "\n",
    "    i_n = A.shape[n-1]\n",
    "    j = U.shape[0]\n",
    "\n",
    "    M_shape = np.asarray(A.shape)\n",
    "    M_shape[n-1] = j\n",
    "    M = np.zeros(M_shape)\n",
    "\n",
    "    for idx, _ in np.ndenumerate(M):\n",
    "        j_idx = idx[n-1]\n",
    "\n",
    "        acc = 0.0\n",
    "        for i in range(i_n):\n",
    "            A_idx = np.asarray(idx)\n",
    "            A_idx[n-1] = i\n",
    "            acc += A[tuple(A_idx)] * U[j_idx, i]\n",
    "\n",
    "        M[idx] = acc\n",
    "\n",
    "    return M"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## N-dimensional Winograd algorithm\n",
    "\n",
    "Using *n-mode Tensor-Matrix product* we can extend Toom-Cook algorithm for *N-dimensional* tensor:\n",
    "$$ (h*x)_{ND} = ((h \\times_{1} G ... \\times_{N} G) \\odot (x \\times_{1} B^{T} ... \\times_{N} B^{T})) \\times_{1} A^{T} ... \\times_{N} A^{T} $$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def conv_wino3d(data, filter, AT, G, BT):\n",
    "    d_transform = nmod_tensor_product(\n",
    "        nmod_tensor_product(\n",
    "            nmod_tensor_product(d, BT, 1),\n",
    "            BT, 2),\n",
    "        BT, 3)\n",
    "\n",
    "    f_transform = nmod_tensor_product(\n",
    "        nmod_tensor_product(\n",
    "            nmod_tensor_product(f, G, 1),\n",
    "            G, 2),\n",
    "        G, 3)\n",
    "\n",
    "    accum = np.multiply(d_transform, f_transform)\n",
    "    x_back_transform = nmod_tensor_product(accum, AT, 1)\n",
    "    y_back_transform = nmod_tensor_product(x_back_transform, AT, 2)\n",
    "    z_back_transform = nmod_tensor_product(y_back_transform, AT, 3)\n",
    "    return z_back_transform\n",
    "\n",
    "def conv_winoNd(data, filter, AT, G, BT, N):\n",
    "    assert N > 0\n",
    "\n",
    "    dim_range = range(2, N+1)\n",
    "\n",
    "    d_transform = nmod_tensor_product(d, BT, 1)\n",
    "    for n in dim_range:\n",
    "        d_transform = nmod_tensor_product(d_transform, BT, n)\n",
    "\n",
    "    f_transform = nmod_tensor_product(f, G, 1)\n",
    "    for n in dim_range:\n",
    "        f_transform = nmod_tensor_product(f_transform, G, n)\n",
    "\n",
    "    accum = np.multiply(d_transform, f_transform)\n",
    "    \n",
    "    back_transform = nmod_tensor_product(accum, AT, 1)\n",
    "    for n in dim_range:\n",
    "        back_transform = nmod_tensor_product(back_transform, AT, n)\n",
    "\n",
    "    return back_transform"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "\n",
    "1. Blahut R.E. Fast algorithms for signal processing\n",
    "2. Fast Algorithms for Convolutional Neural Networks, [URL](https://arxiv.org/pdf/1509.09308.pdf)\n",
    "3. Error Analysis and Improving the Accuracy of Winograd Convolution for Deep Neural Networks, [URL](https://arxiv.org/pdf/1803.10986.pdf)\n",
    "4. Tensor Decompositions and Applications, [URL](https://epubs.siam.org/doi/10.1137/07070111X)\n",
    "5. Deep Tensor Convolution on Multicores, [URL](https://arxiv.org/pdf/1611.06565.pdf)\n",
    "6. High Performance Implementation of 3D Convolutional Neural Networks on a GPU, [URL](https://www.hindawi.com/journals/cin/2017/8348671/)\n",
    "7. Learning Spatiotemporal Features with 3D Convolutional Networks (2015), [URL](https://arxiv.org/pdf/1412.0767.pdf)\n",
    "8. Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset (2018), [URL](https://arxiv.org/pdf/1705.07750.pdf)\n",
    "9. A Closer Look at Spatiotemporal Convolutions for Action Recognition (2018), [URL](https://openaccess.thecvf.com/content_cvpr_2018/papers/Tran_A_Closer_Look_CVPR_2018_paper.pdf)\n",
    "10. Video Classification with Channel-Separated Convolutional Networks (2019), [URL](https://arxiv.org/pdf/1904.02811.pdf)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit"
  },
  "interpreter": {
   "hash": "f8a4220264a22d692c2e3132ffee74066f7b7c05c532ec5d92c4520053df6773"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}